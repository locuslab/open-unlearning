defaults:
  - GradDiff

handler: PDU
method_args:
  retain_loss_eps: ???
  primal_dual: True
  dual_step_size: 1.0
  dual_update_upon: "step" # "step" or "epoch"
  dual_warmup_epochs: 5
  gamma: 1.0
  alpha: 1.0
  loss_names: ["forget_loss", "retain_loss"]

