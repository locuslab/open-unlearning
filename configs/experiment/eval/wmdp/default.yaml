# @package _global_

defaults:
  - override /model: Llama-2-7b-hf
  - override /eval: wmdp

data_split: WMDP-Cyber
retain_logs_path: null

model:
  model_args:
    pretrained_model_name_or_path: open-unlearning/tofu_Llama-3.2-1B-Instruct_full # maybe add Zephyr after RMU

eval:
  wmdp:
    data_split: ${data_split}
    retain_logs_path: ${retain_logs_path}

task_name: ???
